import json
import logging
import asyncio
import re
from typing import Dict, Any, List

logger = logging.getLogger(__name__)


class AdvancedAnalyzer:
    def __init__(self, ollama_client=None):
        if ollama_client is None:
            try:
                import ollama
                self.ollama = ollama.AsyncClient()
            except Exception as e:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å Ollama AsyncClient: {e}")
                self.ollama = None
        else:
            self.ollama = ollama_client

        self.available_models = self._get_available_models()
        self.analysis_cfg = {
            "analysis_timeout": 20,
            "delay_between": 1,
        }

    def _get_available_models(self) -> List[str]:
        try:
            if self.ollama and hasattr(self.ollama, "list"):
                models_response = self.ollama.list()
                installed_models = [m.get("name", "") for m in models_response.get("models", [])]
            else:
                installed_models = []
            possible = [
                "mistral",
                "llama3.2",
                "phi3",
                "qwen2.5",
                "gemma2",
            ]
            available: List[str] = []
            for base in possible:
                for inst in installed_models:
                    if inst.startswith(base):
                        available.append(inst)
                        break
            if not available:
                available = installed_models[:1] if installed_models else ["llama3.2:3b-instruct-q4_K_M"]
            logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª–∏: {available}")
            return available
        except Exception:
            return ["llama3.2:3b-instruct-q4_K_M"]

    def _create_prompt(self, project: Dict[str, Any]) -> str:
        name = project.get("name", "Unknown")
        desc = project.get("description", "No description")
        category = project.get("category", "Unknown")
        tvl = project.get("metrics", {}).get("tvl", 0)
        links = project.get("links", {}) or {}
        links_text = "\n".join(f"{k}: {v}" for k, v in links.items() if v)
        return f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫—Ä–∏–ø—Ç–æ-–ø—Ä–æ–µ–∫—Ç:

–ù–∞–∑–≤–∞–Ω–∏–µ: {name}
–û–ø–∏—Å–∞–Ω–∏–µ: {desc}
–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}
TVL: ${tvl:,.0f}

–û—Ü–µ–Ω–∏ –æ—Ç 1 –¥–æ 10 –∏ –¥–∞–π JSON –æ—Ç–≤–µ—Ç:
{{
  "score": —á–∏—Å–ª–æ 1-10,
  "verdict": "BUY/HOLD/AVOID",
  "summary": "—á—Ç–æ —ç—Ç–æ –∑–∞ –ø—Ä–æ–µ–∫—Ç",
  "confidence": "HIGH/MEDIUM/LOW"
}}
–¢–æ–ª—å–∫–æ JSON.
–°—Å—ã–ª–∫–∏:
{links_text}
"""

    async def analyze_project(self, project: Dict) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–µ–∫—Ç —Å –ø–æ–º–æ—â—å—é LLM."""
        logger.info(f"üîç –ê–Ω–∞–ª–∏–∑: {project.get('name')}")
        try:
            if not self.available_models or not self.ollama:
                return self._fallback(project)

            prompt = self._create_prompt(project)
            response = await self.ollama.chat(
                model=self.available_models[0],
                messages=[{"role": "user", "content": prompt}],
            )
            content = response["message"]["content"]
            logger.info(f"–û—Ç–≤–µ—Ç LLM: {content[:120]}...")
            analysis = json.loads(content)
            return analysis
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return self._fallback(project)

    def _fallback(self, project: Dict[str, Any]) -> Dict[str, Any]:
        return {
            "score": 5.0,
            "verdict": "HOLD",
            "summary": "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞",
            "confidence": "LOW",
        }
